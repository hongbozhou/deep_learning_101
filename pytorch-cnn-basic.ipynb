{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce GTX 860M\n"
     ]
    }
   ],
   "source": [
    "# cuda device name\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "num_epochs = 5\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:06, 1606031.60it/s]                                                                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 139815.25it/s]                                                                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:01, 962854.21it/s]                                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 47085.23it/s]                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset\n",
    "data_root = './data'\n",
    "train_dataset = torchvision.datasets.MNIST(root=data_root,\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root=data_root,\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an iterator to train dataset\n",
    "train_iter = iter(train_loader)\n",
    "# get the first batch of images and their labels\n",
    "dd1, dd2 = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 1, 28, 28]), torch.Size([100]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image size is [100, 1, 28, 28], while label size is [100]\n",
    "dd1.shape, dd2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([28, 28]), tensor(2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The 16th image\n",
    "index = 15\n",
    "# remove the channel dimension\n",
    "bb = dd1[index].squeeze()\n",
    "\n",
    "# the current image dimension (28, 28), the label is digit 4\n",
    "bb.shape, dd2[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#\n",
    "plt.imshow(bb)\n",
    "plt.xticks([])\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # input shape: (None,1,28,28)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2), #output shape: (None,16,28,28)\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)) #output shape: (None,16,14,14)\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2), #output shape: (None,32,14,14)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)) #output shape: (None,32,7,7)\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # input shape: (None,1,28,28)\n",
    "        out = self.layer1(x) #output shape: (None,16,14,14)\n",
    "        out = self.layer2(out) #output shape: (None,32,7,7)\n",
    "        \"\"\"The reshape function also consider the 0th, i.e. batch dimension, different from Keras.Reshape!\"\"\"\n",
    "        out = out.reshape(out.size(0), -1) #output shape: (None,32*7*7)\n",
    "        out = self.fc(out) #output shape: (None,10)\n",
    "        return out\n",
    "\n",
    "model = ConvNet(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.2036\n",
      "Epoch [1/5], Step [200/600], Loss: 0.0700\n",
      "Epoch [1/5], Step [300/600], Loss: 0.0557\n",
      "Epoch [1/5], Step [400/600], Loss: 0.0662\n",
      "Epoch [1/5], Step [500/600], Loss: 0.1829\n",
      "Epoch [1/5], Step [600/600], Loss: 0.0575\n",
      "Epoch [2/5], Step [100/600], Loss: 0.0218\n",
      "Epoch [2/5], Step [200/600], Loss: 0.0131\n",
      "Epoch [2/5], Step [300/600], Loss: 0.0769\n",
      "Epoch [2/5], Step [400/600], Loss: 0.1182\n",
      "Epoch [2/5], Step [500/600], Loss: 0.0752\n",
      "Epoch [2/5], Step [600/600], Loss: 0.0426\n",
      "Epoch [3/5], Step [100/600], Loss: 0.0839\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0045\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0285\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0178\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0145\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0703\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0421\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0271\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0196\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0328\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0341\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0204\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0078\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0081\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0451\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0527\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0162\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0507\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        \"\"\"PyTorch is channel first, different from TensorFlow!\"\"\"\n",
    "        \"\"\"images.size(): torch.Size([100, 1, 28, 28])\"\"\"\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 98.95 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "# has effect on specific layers like Dropout and BatchNorm\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        #Tensor.data: tensor\n",
    "        _, predicted = torch.max(outputs.data, 1)  # (None, 10)\n",
    "        #Tensor.size(): torch.Size, Tensor.size(0): int\n",
    "        total += labels.size(0)\n",
    "        #Tensor.item(): number\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
